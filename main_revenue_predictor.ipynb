{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb520f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms as pth_transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from RevenuePredictorViT import RevenuePredictorViT\n",
    "sys.path.append('dino')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eccb8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_vit = {\"arch\": \"vit_small\",\n",
    " \"patch_size\": 8,\n",
    " \"out_dim\": 65536,\n",
    " \"norm_last_layer\": False,\n",
    " \"warmup_teacher_temp\": 0.04,\n",
    " \"teacher_temp\": 0.07,\n",
    " \"warmup_teacher_temp_epochs\": 30,\n",
    " \"use_fp16\": False,\n",
    " \"weight_decay\": 0.04,\n",
    " \"weight_decay_end\": 0.4,\n",
    " \"clip_grad\": 0,\n",
    " \"batch_size_per_gpu\": 64,\n",
    " \"epochs\": 800,\n",
    " \"freeze_last_layer\": 1,\n",
    " \"lr\": 0.0005,\n",
    " \"warmup_epochs\": 10,\n",
    " \"min_lr\": 1e-05,\n",
    " \"global_crops_scale\": [0.25,\n",
    " 1.0],\n",
    " \"local_crops_scale\": [0.05,\n",
    " 0.25],\n",
    " \"local_crops_number\": 10,\n",
    " \"seed\": 0,\n",
    " \"num_workers\": 10,\n",
    " \"world_size\": 16,\n",
    " \"ngpus\": 8,\n",
    " \"nodes\": 2,\n",
    " \"optimizer\": \"adamw\",\n",
    " \"momentum_teacher\": 0.996,\n",
    " \"use_bn_in_head\": False,\n",
    " \"drop_path_rate\": 0.1,\n",
    " \"image_path\": \"./test_img/airbud.jpeg\",\n",
    " \"threshold\": None,\n",
    " \"image_size\": (480, 480),\n",
    " \"output_dir\": \"./\"\n",
    "}\n",
    "\n",
    "args = {\n",
    "   \"avgpool\": False,\n",
    "    \"last_n_blocks\": 4, # TODO change this, this is what eval linear does\n",
    "    \"n_dnn_img_features\": 16 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74477750",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv (first 10 entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8eaaf",
   "metadata": {},
   "source": [
    "# Test one input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4165078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights found and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.projection_head.0.weight', 'head.projection_head.0.bias', 'head.projection_head.2.weight', 'head.projection_head.2.bias', 'head.projection_head.4.weight', 'head.projection_head.4.bias', 'head.prototypes.weight'])\n",
      "Pretrained weights found and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.projection_head.0.weight', 'head.projection_head.0.bias', 'head.projection_head.2.weight', 'head.projection_head.2.bias', 'head.projection_head.4.weight', 'head.projection_head.4.bias', 'head.prototypes.weight'])\n"
     ]
    }
   ],
   "source": [
    "rp_ViT = RevenuePredictorViT(args, args_vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35897fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to GPU if possible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# load image\n",
    "image_path = \"./test_img/airbud.jpeg\"\n",
    "with open(image_path, 'rb') as f:\n",
    "    img = Image.open(f)\n",
    "    img = img.convert('RGB')\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(args_vit['image_size']),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "img = transform(img)\n",
    "\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % args_vit['patch_size'], img.shape[2] - img.shape[2] % args_vit['patch_size']\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "w_featmap = img.shape[-2] // args_vit['patch_size']\n",
    "h_featmap = img.shape[-1] // args_vit['patch_size']\n",
    "\n",
    "#rp_ViT.ViT_teacher(img.to(device)).shape\n",
    "#intermediate_output = rp_ViT.ViT_teacher.get_intermediate_layers(img.to(device), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e89eed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.tensor([1,2,3])\n",
    "rp_ViT(img, features)\n",
    "\n",
    "#rp_ViT.vit_teacher(imgs)\n",
    "# TODO figure out how to use the dataloader for multiple images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b420e9a",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
