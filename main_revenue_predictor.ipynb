{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb520f15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb520f15",
        "outputId": "454ad1af-f7fc-4120-e90e-ad06e89b2337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'dino'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 168 (delta 54), reused 53 (delta 53), pack-reused 106\u001b[K\n",
            "Receiving objects: 100% (168/168), 24.44 MiB | 15.98 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import re\n",
        "from skimage import io\n",
        "import sys\n",
        "import numpy as np\n",
        "from tqdm import tqdm \n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import transforms as pth_transforms\n",
        "from skimage.transform import resize\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "import torchvision.models as models\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "#from RevenuePredictorViT import RevenuePredictorViT\n",
        "!git clone https://github.com/facebookresearch/dino\n",
        "sys.path.append('dino')\n",
        "from dino import vision_transformer as vits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4006767a",
      "metadata": {
        "id": "4006767a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925580d6",
      "metadata": {
        "id": "925580d6"
      },
      "outputs": [],
      "source": [
        "# set to GPU if possible\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eccb8e25",
      "metadata": {
        "id": "eccb8e25"
      },
      "outputs": [],
      "source": [
        "args_vit = {\"arch\": \"vit_small\",\n",
        " \"patch_size\": 8,\n",
        " \"out_dim\": 65536,\n",
        " \"norm_last_layer\": False,\n",
        " \"warmup_teacher_temp\": 0.04,\n",
        " \"teacher_temp\": 0.07,\n",
        " \"warmup_teacher_temp_epochs\": 30,\n",
        " \"use_fp16\": False,\n",
        " \"weight_decay\": 0.04,\n",
        " \"weight_decay_end\": 0.4,\n",
        " \"clip_grad\": 0,\n",
        " \"batch_size_per_gpu\": 64,\n",
        " \"epochs\": 800,\n",
        " \"freeze_last_layer\": 1,\n",
        " \"lr\": 0.0005,\n",
        " \"warmup_epochs\": 10,\n",
        " \"min_lr\": 1e-05,\n",
        " \"global_crops_scale\": [0.25,\n",
        " 1.0],\n",
        " \"local_crops_scale\": [0.05,\n",
        " 0.25],\n",
        " \"local_crops_number\": 10,\n",
        " \"seed\": 0,\n",
        " \"num_workers\": 10,\n",
        " \"world_size\": 16,\n",
        " \"ngpus\": 8,\n",
        " \"nodes\": 2,\n",
        " \"optimizer\": \"adamw\",\n",
        " \"momentum_teacher\": 0.996,\n",
        " \"use_bn_in_head\": False,\n",
        " \"drop_path_rate\": 0.1,\n",
        " \"image_path\": \"./test_img/airbud.jpeg\",\n",
        " \"threshold\": None,\n",
        " \"image_size\": (480, 480),\n",
        " \"output_dir\": \"./\"\n",
        "}\n",
        "\n",
        "args = {\n",
        "   \"avgpool\": False,\n",
        "    \"last_n_blocks\": 4, # TODO change this, this is what eval linear does\n",
        "    \"n_dnn_img_features\": 16,\n",
        "    \"n_features\": 25,\n",
        "    \"n_output_bins\": 6,\n",
        "}\n",
        "\n",
        "path_prefix = './'\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    n_max = None\n",
        "else:\n",
        "    n_max = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87902e5",
      "metadata": {
        "id": "a87902e5"
      },
      "source": [
        "Optional: for running on google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f78c7b4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f78c7b4c",
        "outputId": "1f68b898-caf8-4a89-abe8-a2c9d1c9994d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    path_prefix = '/content/drive/My Drive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c0f5ea",
      "metadata": {
        "id": "40c0f5ea"
      },
      "source": [
        "# Load Data\n",
        "Parameters\n",
        "- csv\n",
        "- number of entries to load (default all)\n",
        "- path to images\n",
        "- image transform function\n",
        "\n",
        "Returns\n",
        "- imgs\n",
        "- features\n",
        "- revenue (label?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec2626a8",
      "metadata": {
        "id": "ec2626a8"
      },
      "outputs": [],
      "source": [
        "def img_rename(img_name):\n",
        "    return re.sub(r'\\W+', ' ', img_name).lower().strip().replace(' ', '+') + '.jpg'\n",
        "\n",
        "def resize_poster(img, target_size):\n",
        "    img_resized = resize(img, (target_size, int(img.shape[1] * (target_size / img.shape[0]))), anti_aliasing=True)\n",
        "    pad_size_1 = (target_size - img_resized.shape[1]) // 2\n",
        "    pad_size_2 = target_size - img_resized.shape[1] - pad_size_1\n",
        "    padding = [(0, 0), (pad_size_1, pad_size_2), (0, 0)]\n",
        "    if len(img_resized.shape) == 2: # gray scale image\n",
        "        padding = [(0, 0), (pad_size_1, pad_size_2)]\n",
        "    img_padded = np.pad(img_resized, padding, mode='constant', constant_values=0)\n",
        "    return img_padded\n",
        "\n",
        "def get_genre_dict():\n",
        "    api_key_auth3 = 'fad9ac13c7b36b3e05f6b63be16e74f0'\n",
        "    genre_dictionary = {}\n",
        "\n",
        "    response = requests.get(f\"https://api.themoviedb.org/3/genre/movie/list?api_key={api_key_auth3}&language=en-US\")\n",
        "    response = response.json()\n",
        "\n",
        "    for genre in response['genres']:\n",
        "        genre_dictionary[genre['id']] = genre['name']\n",
        "        \n",
        "    return genre_dictionary\n",
        "\n",
        "def expand_genres_id(df):\n",
        "    genre_dictionary = get_genre_dict()\n",
        "    \n",
        "    temp_df = df[['id', 'genre_ids']]\n",
        "    temp_df['genre_ids'] = [ast.literal_eval(x) for x in temp_df['genre_ids']]\n",
        "    temp_df['genres'] = [[genre_dictionary[v] for v in g_array] for g_array in temp_df['genre_ids']]\n",
        "\n",
        "    exploded_df = temp_df[['id', 'genres']].explode(column='genres')\n",
        "    exploded_df = pd.get_dummies(exploded_df, columns=['genres']).groupby('id', as_index=False).sum()\n",
        "\n",
        "    merged_features = pd.merge(left=df\n",
        "                    , right=exploded_df\n",
        "                    , left_on='id'\n",
        "                    , right_on='id')\n",
        "    \n",
        "    return merged_features.drop(columns=['genre_ids'])\n",
        "\n",
        "def normalize_columns(df, cols):\n",
        "    # Replace na with col mean\n",
        "    df[cols] = df[cols].fillna(df[cols].mean())\n",
        "    \n",
        "    # Normalize\n",
        "    c_min = df[cols].min()\n",
        "    c_max = df[cols].max()\n",
        "    df[cols]=(df[cols]-c_min)/(c_max-c_min)\n",
        "    return df\n",
        "\n",
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform, device, n_output_bins, n_max=None):\n",
        "        # load csv data\n",
        "        md_df = pd.read_csv(csv_path)\n",
        "        \n",
        "        # TODO TRANSFORM FEATURES FROM CSV SOMEHOW\n",
        "        ## Genres\n",
        "        md_df = expand_genres_id(md_df)\n",
        "        \n",
        "        ## Normalize and fill na\n",
        "        to_normalize = ['popularity', 'vote_average', 'vote_count', 'production_budget', 'running_time']\n",
        "        md_df = normalize_columns(md_df, to_normalize)\n",
        "        \n",
        "        \n",
        "        # Groups numerical revenue, currently 6 bins\n",
        "        num = range(n_output_bins)\n",
        "\n",
        "        revenue_in = pd.qcut(md_df['domestic_box_office'] + md_df['international_box_office'], q=n_output_bins, labels=num)\n",
        "\n",
        "        enc = OneHotEncoder().fit(np.array(revenue_in).reshape(-1,1))\n",
        "        df = pd.DataFrame(enc.transform(np.array(revenue_in).reshape(-1,1)).toarray())\n",
        "        df['combine'] = df.values.tolist()\n",
        "        md_df['revenue_bin'] = df['combine']\n",
        "        \n",
        "        ## Drop a few columns \n",
        "        to_drop = ['adult', 'production_year', 'domestic_box_office', 'international_box_office'\n",
        "                   , 'genre', 'original_language', 'id']\n",
        "        md_df = md_df.drop(columns=to_drop)\n",
        "\n",
        "        print(md_df.columns)\n",
        "        \n",
        "        movie_data = np.pad(md_df.to_numpy(), [(0, 0), (0, 1)], mode='constant', constant_values=np.nan)\n",
        "        \n",
        "        # restrict dataset for testing purposes\n",
        "        if n_max:\n",
        "            movie_data = movie_data[:n_max,:]\n",
        "        \n",
        "        # load image data\n",
        "        self.movie_name_id = 3\n",
        "        \n",
        "        for i in tqdm(range(movie_data.shape[0])):\n",
        "            img_path = img_dir + img_rename(movie_data[i,self.movie_name_id])\n",
        "            try:\n",
        "                img = transform(io.imread(img_path))\n",
        "                if len(img.shape) < 3: # if greyscale, make rgb\n",
        "                    img = np.stack((img, img, img), axis=-1) \n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "            movie_data[i, -1] = img\n",
        "\n",
        "        self.movie_data = movie_data\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.movie_data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        movie = self.movie_data[idx]\n",
        "        \n",
        "        revenue_id = 10\n",
        "        \n",
        "        img = torch.tensor(movie[-1]).to(torch.float32).swapaxes(0,2)\n",
        "        label = torch.tensor(movie[-2]) # set this to whatever the revenue entry is\n",
        "        features = np.delete(movie, (self.movie_name_id, -2, -1)) # remove image and revenue from non-image features\n",
        "        features = torch.tensor(features.astype(float)).to(torch.float32) \n",
        "        \n",
        "        # TODO idk how we're representing the dnn features so I'm just gonna make a dummy\n",
        "        #features = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14]).to(torch.float32)\n",
        "        #features = movie.drop(columns=to_drop)\n",
        "        \n",
        "        # MAKE THE SIZES WORK FOR VIT\n",
        "        #img = img.swapaxes(0,2).unsqueeze(0).to(torch.float32)\n",
        "        #features = features.unsqueeze(0)\n",
        "        \n",
        "        return img, features, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ce990744",
      "metadata": {
        "id": "ce990744"
      },
      "outputs": [],
      "source": [
        "def load_dataset(csv_name, poster_dir, device):\n",
        "    # TODO change transform function if desired\n",
        "    img_size = 256\n",
        "    transform = lambda x: resize_poster(x, img_size) \n",
        "\n",
        "    dataset = FeatureDataset(\n",
        "        csv_path=path_prefix+csv_name,\n",
        "        img_dir=path_prefix+poster_dir,\n",
        "        transform=transform,\n",
        "        device=device,\n",
        "        n_max=n_max,\n",
        "        n_output_bins=args['n_output_bins']\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def split_dataset(dataset, batch_size):\n",
        "    test_pct, val_pct = 0.3, 0.1\n",
        "    test_size = int(len(dataset)*test_pct)\n",
        "    dataset_size = len(dataset) - test_size\n",
        "    val_size = int(dataset_size*val_pct)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    val_dl = DataLoader(val_dataset, batch_size, num_workers=0, pin_memory=True)\n",
        "    test_dl = DataLoader(test_dataset, batch_size, num_workers=0, pin_memory=True)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, train_dl, val_dl, test_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77869e1c",
      "metadata": {
        "id": "77869e1c"
      },
      "source": [
        "# Generic Training Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "52c66d33",
      "metadata": {
        "id": "52c66d33"
      },
      "outputs": [],
      "source": [
        "def accuracies_aphr(pred, label):\n",
        "\n",
        "  _, preds = torch.max(pred, dim=1)\n",
        "  _, gts = torch.max(label, dim=1)\n",
        "\n",
        "  aphr_bingo = torch.tensor(torch.sum(preds == gts).item() / len(preds))\n",
        "  next_1 = torch.sum(preds == (gts + 1)).item()\n",
        "  prev_1 = torch.sum(preds == (gts - 1)).item()\n",
        "\n",
        "  aphr_1_away = torch.tensor(aphr_bingo + next_1/len(preds) + prev_1/len(preds))\n",
        "\n",
        "  return aphr_bingo.detach(), aphr_1_away.detach()\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    _, gts = torch.max(labels, dim=1)\n",
        "\n",
        "    return torch.tensor(torch.sum(preds == gts).item() / len(preds))\n",
        "    \n",
        "class DNNBase(nn.Module):\n",
        "    # training step\n",
        "    def training_step(self, batch):\n",
        "        img, features, targets = batch\n",
        "        \n",
        "        img = img.to(device)\n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        out = self(img, features)\n",
        "        loss = F.cross_entropy(out, targets)\n",
        "        acc = accuracy(out, targets)\n",
        "        return loss, acc\n",
        "    \n",
        "    # validation step\n",
        "    def validation_step(self, batch):\n",
        "        img, features, targets = batch\n",
        "        \n",
        "        img = img.to(device)\n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        out = self(img, features)\n",
        "        loss = F.cross_entropy(out, targets)\n",
        "        acc = accuracy(out, targets)\n",
        "\n",
        "        aphr_bingo, aphr_1_away = accuracies_aphr(out, targets)\n",
        "        return {'val_acc':acc.detach(), 'val_loss':loss.detach(), 'val_aphr_bingo':aphr_bingo, 'val_aphr_1_away':aphr_1_away}\n",
        "    \n",
        "    # validation epoch end\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()\n",
        "\n",
        "        batch_aphr_bingo = [x['val_aphr_bingo'] for x in outputs]\n",
        "        epoch_aphr_bingo = torch.stack(batch_aphr_bingo).mean()\n",
        "        batch_aphr_1_away = [x['val_aphr_1_away'] for x in outputs]\n",
        "        epoch_aphr_1_away = torch.stack(batch_aphr_1_away).mean()\n",
        "        return {'val_loss':epoch_loss.item(), 'val_acc':epoch_acc.item(), 'val_aphr_bingo':epoch_aphr_bingo.item(), 'val_aphr_1_away':epoch_aphr_1_away.item()}\n",
        "        \n",
        "    # print result end epoch\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}] (lr={:.4f}): train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, val_aphr_1_away: {:.4f}, test_acc: {:.4f}, test_aphr_1_away: {:.4f}\".format(\n",
        "                epoch, result[\"lr\"], result[\"train_loss\"], result[\"train_acc\"], result[\"val_loss\"], result[\"val_acc\"], result[\"val_aphr_1_away\"], result[\"test_acc\"], result[\"test_aphr_1_away\"]\n",
        "            ))\n",
        "\n",
        "        \n",
        "class DNN_module(nn.Module):\n",
        "    def __init__(self, input_dim, num_features, output_dim = 6, dropout = 0.6):\n",
        "\n",
        "        unit_1 = 72\n",
        "        unit_2 = 128\n",
        "        unit_3 = 256\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, num_features)\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(num_features, unit_1),\n",
        "            nn.ReLU(inplace=False),   \n",
        "            nn.Dropout(p=dropout),\n",
        "\n",
        "            nn.Linear(unit_1, unit_2),\n",
        "            nn.ReLU(inplace=False),   \n",
        "            nn.Dropout(p=dropout),\n",
        "\n",
        "            nn.Linear(unit_2, unit_3),\n",
        "            nn.ReLU(inplace=False),   \n",
        "            nn.Dropout(p=dropout),\n",
        "\n",
        "            nn.Linear(unit_3, unit_2),\n",
        "            nn.ReLU(inplace=False),   \n",
        "            nn.Dropout(p=dropout),\n",
        "\n",
        "            nn.Linear(unit_2, unit_1),\n",
        "            nn.ReLU(inplace=False),   \n",
        "            nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(unit_1, output_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, features):\n",
        "        x = self.input_layer(features)\n",
        "        x = self.layers(x)\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4421d7f",
      "metadata": {
        "id": "c4421d7f"
      },
      "outputs": [],
      "source": [
        "class PreTrainedResnet18(DNNBase):\n",
        "    def __init__(self, args, freeze_cnn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.args = args\n",
        "        \n",
        "        self.cnn = models.resnet18(pretrained=True)\n",
        "        # Replace last layer\n",
        "        num_ftrs = self.cnn.fc.in_features\n",
        "        self.cnn.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, args['n_dnn_img_features'])\n",
        "        )\n",
        "\n",
        "        # freeze cnn\n",
        "        if freeze_cnn:\n",
        "            for param in self.cnn.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.cnn.fc.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        # TODO add DNN layers here\n",
        "        self.dnn = DNN_module(args['n_dnn_img_features'] + args['n_features'] - 1\n",
        "                              , 2 + args['n_features']\n",
        "                              , args['n_output_bins'])\n",
        "        \n",
        "    def forward(self, img, features):\n",
        "        # CNN output\n",
        "        cnn_output = self.cnn(img)\n",
        "        dnn_input = torch.cat((cnn_output, features), dim=1)\n",
        "        output = self.dnn(dnn_input)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efbe7236",
      "metadata": {
        "id": "efbe7236"
      },
      "outputs": [],
      "source": [
        "class RevenuePredictorViT(DNNBase):\n",
        "    def __init__(self, args, args_vit, freeze_vit=False):\n",
        "        super().__init__()\n",
        "\n",
        "        student, teacher = self.load_pretrained_ViT(args_vit)\n",
        "\n",
        "        self.vit_student = student\n",
        "        self.vit_teacher = teacher\n",
        "\n",
        "        self.vit_output_size = \\\n",
        "            args['last_n_blocks'] * self.vit_teacher.embed_dim\n",
        "\n",
        "        #self.vit2dnn = nn.Linear(self.vit_output_size, args['n_dnn_img_features'])\n",
        "        self.vit2dnn = nn.Sequential(\n",
        "            nn.Linear(self.vit_output_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, args['n_dnn_img_features'])\n",
        "        )\n",
        "\n",
        "        # TODO set this to actual DNN\n",
        "        #self.dnn = nn.Linear(args['n_dnn_img_features'] + args['n_features'], 1)\n",
        "        self.dnn = DNN_module(args['n_dnn_img_features'] + args['n_features'] - 1\n",
        "                              , 2 + args['n_features']\n",
        "                              , args['n_output_bins'])\n",
        "\n",
        "        self.args = args\n",
        "        \n",
        "        if freeze_vit:\n",
        "            for param in self.vit_teacher.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.vit_student.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, img, features, freeze_vit=True):\n",
        "        # vit\n",
        "        model = self.vit_teacher\n",
        "        intermediate_output = model.get_intermediate_layers(img.swapaxes(-1,-2), self.args['last_n_blocks'])\n",
        "        vit_output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
        "        if self.args['avgpool']:\n",
        "            vit_output = torch.cat((vit_output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
        "            vit_output = vit_output.reshape(vit_output.shape[0], -1)\n",
        "\n",
        "        # vit2dnn\n",
        "        fc_output = self.vit2dnn(vit_output)\n",
        "\n",
        "        # dnn\n",
        "        dnn_input = torch.cat((fc_output, features), dim=1) # concatenate img features with other movie details\n",
        "        output = self.dnn(dnn_input)\n",
        "        return output\n",
        "\n",
        "    # create function to load pretrained model\n",
        "    def load_pretrained_ViT(self, args):\n",
        "\n",
        "        # initialize models\n",
        "        student = vits.__dict__[args['arch']](patch_size=args['patch_size'], num_classes=0)\n",
        "        teacher = vits.__dict__[args['arch']](patch_size=args['patch_size'], num_classes=0)\n",
        "\n",
        "        # fetch pretrained models\n",
        "        url = None\n",
        "        if args['arch'] == \"vit_small\" and args['patch_size'] == 16:\n",
        "            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain_full_checkpoint.pth\"\n",
        "        elif args['arch'] == \"vit_small\" and args['patch_size'] == 8:\n",
        "            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain_full_checkpoint.pth\"  # model used for visualizations in our paper\n",
        "        elif args['arch'] == \"vit_base\" and args['patch_size'] == 16:\n",
        "            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain_full_checkpoint.pth\"\n",
        "        elif args['arch'] == \"vit_base\" and args['patch_size'] == 8:\n",
        "            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain_full_checkpoint.pth\"\n",
        "\n",
        "        state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
        "\n",
        "        state_dict_teacher = state_dict['teacher']\n",
        "        # remove `module.` prefix\n",
        "        state_dict_teacher = {k.replace(\"module.\", \"\"): v for k, v in state_dict_teacher.items()}\n",
        "        # remove `backbone.` prefix induced by multicrop wrapper\n",
        "        state_dict_teacher = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict_teacher.items()}\n",
        "\n",
        "        msg = teacher.load_state_dict(state_dict_teacher, strict=False)\n",
        "        print('Pretrained weights found and loaded with msg: {}'.format(msg))\n",
        "\n",
        "        state_dict_student = state_dict['student']\n",
        "        # remove `module.` prefix\n",
        "        state_dict_student = {k.replace(\"module.\", \"\"): v for k, v in state_dict_student.items()}\n",
        "        # remove `backbone.` prefix induced by multicrop wrapper\n",
        "        state_dict_student = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict_student.items()}\n",
        "\n",
        "        msg = student.load_state_dict(state_dict_student, strict=False)\n",
        "        print('Pretrained weights found and loaded with msg: {}'.format(msg))\n",
        "\n",
        "        # test this by running the eval thing\n",
        "        return student, teacher"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66c3618",
      "metadata": {
        "id": "e66c3618"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "6000a1e5",
      "metadata": {
        "id": "6000a1e5"
      },
      "outputs": [],
      "source": [
        "def fit_one_cycle(model, device, train_loader, val_loader, test_loader, opt_func, train_args, lr_scheduling):\n",
        "    lr, batch_size, epochs = train_args['lr'], train_args['batch_size'], train_args['epochs']\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    if lr_scheduling == 'cosine':\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-3)\n",
        "    elif lr_scheduling == 'step':\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "    elif lr_scheduling == 'plateau':\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()  \n",
        "        train_losses = []\n",
        "        train_accs = []\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            loss, acc = model.training_step(data)\n",
        "\n",
        "            train_losses.append(loss)\n",
        "            train_accs.append(acc)\n",
        "            \n",
        "            # calculates gradients\n",
        "            loss.backward()\n",
        "            # perform gradient descent and modifies the weights\n",
        "            optimizer.step()\n",
        "            # clear grads\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        # Validation and Test phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        test_acc = evaluate(model, test_loader)#['val_acc']\n",
        "\n",
        "        # set lr\n",
        "        if lr_scheduling:\n",
        "            if lr_scheduling == 'plateau':\n",
        "                scheduler.step(result['val_loss'])\n",
        "            else:\n",
        "                scheduler.step()\n",
        "            result['lr'] = optimizer.param_groups[0]['lr']\n",
        "        else:\n",
        "            result['lr'] = None\n",
        "\n",
        "        result['train_acc'] = torch.stack(train_accs).mean().item()\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['test_acc'] = test_acc['val_acc']\n",
        "        result['test_aphr_1_away'] = test_acc['val_aphr_1_away']\n",
        "        model.epoch_end(epoch, result)\n",
        "\n",
        "        history.append(result)\n",
        "        \n",
        "    return history\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def plot_history(history):\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    test_acc = []\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    lrs = []\n",
        "    val_aphr_1_away, test_aphr_1_away = [],[]\n",
        "    time = list(range(len(history)))\n",
        "    for h in history:\n",
        "        train_acc.append(h['train_acc'])\n",
        "        val_acc.append(h['val_acc'])\n",
        "        test_acc.append(h['test_acc'])\n",
        "        train_loss.append(h['train_loss'])\n",
        "        val_loss.append(h['val_loss'])\n",
        "        val_aphr_1_away.append(h['val_aphr_1_away'])\n",
        "        test_aphr_1_away.append(h['test_aphr_1_away'])\n",
        "        lrs.append(h['lr'])\n",
        "\n",
        "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(18, 4))\n",
        "    ax1.plot(time, train_loss, c='red', label='train_loss')\n",
        "    ax1.plot(time, val_loss, c='blue', label='val_loss')\n",
        "    ax1.set_title('Train/val Loss vs. Epochs')\n",
        "    ax1.legend()\n",
        "    ax2.plot(time, train_acc, c='red', label='train_acc')\n",
        "    ax2.plot(time, val_acc, c='blue', label='val_acc')\n",
        "    ax2.plot(time, test_acc, c='green', label='test_acc')\n",
        "    ax2.set_title('Exact Accuracies vs. Epochs')\n",
        "    ax2.legend()\n",
        "    ax3.plot(time, val_aphr_1_away, c='blue', label='val_aphr_1_away')\n",
        "    ax3.plot(time, test_aphr_1_away, c='green', label='test_aphr_1_away')\n",
        "    ax3.set_title('APHR_1_away vs. Epochs')\n",
        "    ax3.legend()\n",
        "    ax4.plot(time, lrs, c='red')\n",
        "    ax4.set_title('Learning Rate vs Epochs')\n",
        "    # fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8135ce2",
      "metadata": {
        "id": "b8135ce2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7428685e",
      "metadata": {
        "id": "7428685e"
      },
      "outputs": [],
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "    train_args = {\n",
        "        \"batch_size\": 128,\n",
        "        \"lr\": 5e-3,\n",
        "        \"epochs\": 50,\n",
        "        \"report_every\": 10\n",
        "    }\n",
        "else:\n",
        "    train_args = {\n",
        "        \"batch_size\": 10,\n",
        "        \"lr\": 1e-3,\n",
        "        \"epochs\": 2,\n",
        "        \"report_every\": 1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9435614d",
      "metadata": {
        "id": "9435614d"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f05ecfbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f05ecfbe",
        "outputId": "186e5d03-c9a8-4020-f31c-aeb0740d2249"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-cb1ac167bf27>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp_df['genre_ids'] = [ast.literal_eval(x) for x in temp_df['genre_ids']]\n",
            "<ipython-input-7-cb1ac167bf27>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp_df['genres'] = [[genre_dictionary[v] for v in g_array] for g_array in temp_df['genre_ids']]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['popularity', 'vote_average', 'vote_count', 'movie_name',\n",
            "       'production_budget', 'running_time', 'genres_Action',\n",
            "       'genres_Adventure', 'genres_Animation', 'genres_Comedy', 'genres_Crime',\n",
            "       'genres_Documentary', 'genres_Drama', 'genres_Family', 'genres_Fantasy',\n",
            "       'genres_History', 'genres_Horror', 'genres_Music', 'genres_Mystery',\n",
            "       'genres_Romance', 'genres_Science Fiction', 'genres_TV Movie',\n",
            "       'genres_Thriller', 'genres_War', 'genres_Western', 'revenue_bin'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [05:59<00:00,  3.91it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('MovieDataEnhanced.zip', 'poster_img/', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "40d77106",
      "metadata": {
        "id": "40d77106"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset, \\\n",
        "    train_dl, val_dl, test_dl = split_dataset(dataset, train_args['batch_size'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e130cd",
      "metadata": {
        "id": "71e130cd"
      },
      "source": [
        "### Run training cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "7454fa0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7454fa0f",
        "outputId": "d8239053-d3f3-495d-bab5-03287ab764c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "rp_CNN = PreTrainedResnet18(args, freeze_cnn=True)\n",
        "rp_CNN = rp_CNN.to(device) # send model to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "609de345",
      "metadata": {
        "id": "609de345"
      },
      "outputs": [],
      "source": [
        "cosine_history = fit_one_cycle(rp_CNN, device, train_dl, val_dl, test_dl, torch.optim.Adam, {\n",
        "    \"batch_size\": 128,\n",
        "    \"lr\": 0.01,\n",
        "    \"epochs\": 100,\n",
        "    }, 'cosine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "odSbyOPvOoGV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odSbyOPvOoGV",
        "outputId": "3fc30188-ae87-4d4f-c293-f1a002489529"
      },
      "outputs": [],
      "source": [
        "plateau_history = fit_one_cycle(rp_CNN, device, train_dl, val_dl, test_dl, torch.optim.Adam, {\n",
        "    \"batch_size\": 128,\n",
        "    \"lr\": 0.01,\n",
        "    \"epochs\": 100,\n",
        "    }, 'plateau')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c45dcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "d7c45dcb",
        "outputId": "863b8828-fc98-4612-d7c5-cddb32f8cd62"
      },
      "outputs": [],
      "source": [
        "plot_history(cosine_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51624350",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51624350",
        "outputId": "d530e480-bbf7-40d6-bcda-9fbf1eaf4a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained weights found and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.projection_head.0.weight', 'head.projection_head.0.bias', 'head.projection_head.2.weight', 'head.projection_head.2.bias', 'head.projection_head.4.weight', 'head.projection_head.4.bias', 'head.prototypes.weight'])\n",
            "Pretrained weights found and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.projection_head.0.weight', 'head.projection_head.0.bias', 'head.projection_head.2.weight', 'head.projection_head.2.bias', 'head.projection_head.4.weight', 'head.projection_head.4.bias', 'head.prototypes.weight'])\n"
          ]
        }
      ],
      "source": [
        "rp_ViT = RevenuePredictorViT(args, args_vit, freeze_vit=True)\n",
        "rp_ViT = rp_ViT.to(device) # send model to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd85c3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abd85c3f",
        "outputId": "86a65dd5-e476-4e22-8c92-378c01233abb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-111-72658b2e4deb>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aphr_1_away = torch.tensor(aphr_bingo + next_1/len(preds) + prev_1/len(preds))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0] : train_loss: 1.7920, train_acc: 0.1569, val_loss: 1.7916, val_acc: 0.1633, val_aphr_1_away: 0.4388, test_acc: 0.1657, test_aphr_1_away: 0.5017\n"
          ]
        }
      ],
      "source": [
        "train_vit = fit_one_cycle(rp_ViT, device, train_dl, val_dl, test_dl, torch.optim.Adam, train_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3253d491",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "3253d491",
        "outputId": "4545c006-1461-4ae8-981b-b6fe5a593855"
      },
      "outputs": [],
      "source": [
        "plot_history(train_vit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119e4dab",
      "metadata": {
        "id": "119e4dab"
      },
      "source": [
        "# Visualize Attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce1ec08",
      "metadata": {
        "id": "5ce1ec08"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(model, args, img):\n",
        "\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "        \n",
        "    w_featmap = img.shape[-2] // args['patch_size']\n",
        "    h_featmap = img.shape[-1] // args['patch_size']\n",
        "\n",
        "    attentions = model.get_last_selfattention(img.swapaxes(-1,-2).to(device))\n",
        "\n",
        "    nh = attentions.shape[1] # number of head\n",
        "\n",
        "    # we keep only the output patch attention\n",
        "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "\n",
        "    if args['threshold'] is not None:\n",
        "        # we keep only a certain percentage of the mass\n",
        "        val, idx = torch.sort(attentions)\n",
        "        val /= torch.sum(val, dim=1, keepdim=True)\n",
        "        cumval = torch.cumsum(val, dim=1)\n",
        "        th_attn = cumval > (1 - args['threshold'])\n",
        "        idx2 = torch.argsort(idx)\n",
        "        for head in range(nh):\n",
        "            th_attn[head] = th_attn[head][idx2[head]]\n",
        "        th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
        "        # interpolate\n",
        "        th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=args['patch_size'], mode=\"nearest\")[0].cpu().numpy()\n",
        "\n",
        "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
        "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=args['patch_size'], mode=\"nearest\")[0].cpu().numpy()\n",
        "\n",
        "    # save attentions heatmaps\n",
        "    os.makedirs(args['output_dir'], exist_ok=True)\n",
        "    torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join(args['output_dir'], \"img.png\"))\n",
        "    for j in range(nh):\n",
        "        fname = os.path.join(args['output_dir'], \"attn-head\" + str(j) + \".png\")\n",
        "        plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
        "        print(f\"{fname} saved.\")\n",
        "        plt.imshow(attentions[j])\n",
        "        plt.show()\n",
        "\n",
        "    if args['threshold'] is not None:\n",
        "        image = skimage.io.imread(os.path.join(args['output_dir'], \"img.png\"))\n",
        "        for j in range(nh):\n",
        "            display_instances(image, th_attn[j], fname=os.path.join(args['output_dir'], \"mask_th\" + str(args['threshold']) + \"_head\" + str(j) +\".png\"), blur=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d41a167",
      "metadata": {
        "id": "5d41a167",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "img, features, label = train_dataset[3]\n",
        "print(img.shape)\n",
        "print(features.shape)\n",
        "print(label)\n",
        "plt.imshow(img.swapaxes(0,2).detach().numpy())\n",
        "plt.show()\n",
        "img = img.unsqueeze(0).to(torch.float32)\n",
        "features = features.unsqueeze(0)\n",
        "visualize_attention(rp_ViT.vit_teacher, args_vit, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d48d7b3",
      "metadata": {
        "id": "0d48d7b3",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
